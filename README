Below are instructions to run TiKV-XLL as described in our NSDI 2026
paper "XLL: Cross-Layer Logging for Data Deduplication in
Consensus-Based Storage."

Step 0 - System requirements
~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The published setup scripts assume a cluster of five Cloudlab r6615
(Clemson) nodes running Ubuntu 22.04. We expect the artifact to run on
other Linux distributions, however, different compiler versions could
result in build errors when compiling the project.

The hardware configuration used in the paper is:
- Two NVMe devices, /dev/nvme0n1 and /dev/nvme1n1.
- /dev/nvme1n1 is an empty disk on r6615 machines.
- We create an ext4 partition on /dev/nvme1n1 mounted to /mnt/data to
  hold experimental data (key-value data written by TiKV).

Other configurations are acceptable -- for example two NVMe disks are
not necessary to run the artifact.

The ansible setup script (see Step 1) has commented sections which can
be used to create disk partitions and copy ssh keys to the experiment
hosts. These should be edited appropriately and uncommented before
running the script.

To run the ansible script in Step 1, you will need to install
ansible-playbook on a "control node" (i.e., personal computer). The
script will install all the software for the experiments on a set of
"managed nodes" in the experiment (i.e., cloudlab hosts). Thus, the
list of node hostnames (or, ssh targets) in the experiment should be
in inventory.yml. These could be actual hostnames or nicknames defined
in ~/.ssh/config.

See the following links for further instruction on installing ansible
and writing the inventory.yml file.
- https://docs.ansible.com/projects/ansible/latest/installation_guide/index.html
- https://docs.ansible.com/projects/ansible/latest/inventory_guide/index.html

If you don't wish to use ansible, read through the script to see which
dependencies are needed to run the artifact. You will need to install
them manually if not using the script.

Step 1 - Set up
~~~~~~~~~~~~~~~

We provide an ansible playbook to install required dependencies,
download artifact repos, and configure TiKV-XLL and client
nodes. Requirements to run the script are listed above in Step 0.

To run the playbook:

ansible-playbook -i inventory.yml setup-cloudlab-nvme.yml

Ansible will clone artifact repositories from github. Because the
artifact is very large, with several dependencies, it was not 
feasible to package the artifact in a single repository.

Step 2 - TiKV, XLL-SO, and XLL
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The three main versions of TiKV evaluated in the paper are available
as separate branches of the xll github repository:
- "master": Baseline TiKV 5.4.3 with no additional modifications.
- "tikv-xll-so": "Separation Only" stores data payloads in two XLL instances.
- "tikv-xll": stores data payloads in a single "deduplicated" XLL instance.

We also include branches "tikv-xll-gc", which has the XLL garbage collector
and "tikv-xll-slow-read" which omits the read optimization described in the
paper.

To evaluate a particular version:
- git checkout <branch>
- cargo build --release
- run experiment script (see Step 3)

Step 3 - Running Experiments
~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The experiment script tikv-ycsb.py:
- starts TiKV and PD services on storage nodes.
- runs go-ycsb, the benchmark program on the client nodes.
- gathers output and shuts down TiKV and PD services once finished.

As an example, this invocation runs the complete set of ycsb workloads
using tikv-ycsb.py:

./tikv-ycsb.py --tikv_nodes 10.10.1.2,10.10.1.3,10.10.1.4 --pd_node 10.10.1.1 \
--client_node 10.10.1.5 -s $((100 * 1024 * 1024 * 1024)) -v 16384 \
--workloads a,b,c,d,e,f --name tikv-ycsb --experimenttype ycsb

The script accepts a comma separated list of TiKV hosts, a PD node,
and a comma separated list of client nodes. A complete list of flags
and arguments can be found in the python script. This example loads
the key-value store with 100GB of data using 16KB values and runs YCSB
workloads A, B, C, D, E, F in that order. Most of the YCSB experiments
should be run with --experimenttime ycsb.

If you pass --threadsmin and --threads as arguments to tikv-ycsb.py,
the script will run the workloads over an increasing range of
threads. This is used for scalability workloads. If more than one
client node is given, equal proportions of threads will be run on each
client. For write scalability experiments, use `--experimenttype
writescalability`, since these experiments should start with a clean
DB each time. For read scalability experiments, use `--experimenttype
ycsb` and `--workloads c`.

tikv-ycsb.py creates an output directory with this naming convention:
    <name>-<dbsize>-<valuesize>
Names themselves are arbitrary, but our graphing scripts expect names 
to be formatted like this.

We generally use <dbtype>-<experiment> for the name. For example, to 
evaluate YCSB on all three systems, you would use names "tikv-ycsb", 
"xllso-ycsb", and "xll-ycsb". The example above creates a directory
named "tikv-ycsb-100GB-16KB". 

Step 4 - Understanding Results
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

tikv-ycsb.py stores all output in a `results/` directory. Output from
each experiment is copied to a subdirectory of `results/`. The name of
the subdirectory is a combination of the experiment name, DB size, and
value size. The numbered directories (0/, 1/, 2/) store per-node output
like stdout logs and TiKV metrics. The .ycsb files are client go-ycsb 
logs. YCSB data is taken from the end summary of each ycsb log.

Note: we changed the name of the layer-deduplicated version of TiKV at
some point from "wotr" to "XLL". Therefore, you might see "wotr"
mentioned in some of scripts or source code. "wotr" and "XLL" are the 
same thing, it is simply a name change.

Figure 7 is generated from fine-grained metrics captured at the end of
an experiment. plotting/waterfall-data.py produces Figure
7. The experiment names in the python script should match the names of
directories in the results folder created by tikv-ycsb.py.

Figures 8-12 are all YCSB workloads, thus the experiments should be
run with tikv-ycsb.py. The graphs can be produced with scripts in
plotting/. Unfortunately the graphing scripts are fragile with respect
to experiment names. We've added some comments at the top of each 
script to clarify the expected experiment names so the graphs can be
reproduced more easily.

The graphing scripts store intermediate data in *.dat files and produce
pdf output in ../graphs. See `common.sh` for more detail if needed.

Step 5 - LPFS Experiments (Figures 14-16)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

LPFS is included in the archive lpfs-artifact.tar.gz. Untar the
archive and follow instructions in the README to compile LPFS. The
README includes instructions for running the experiments in the paper.

If you wish to mount LPFS manually, follow the example in the
experiment script to mount the FUSE filesystem. LPFS is not a
production filesystem, thus some FUSE callbacks are missing. It has
enough functionality to run LevelDB.
